##  DANet: Dual-attention Network for View-invariant Action  recognition.  

#we propose a Dual-Attention Network (DANet) aims to learn robust video representation for view-invariant action recognition. 
The DANet is composed of relation-aware spatiotemporal self-attention and spatiotemporal cross-attention modules.
The relation-aware spatiotemporal self-attention module learns representative and discriminative action features. This module
captures local and global long-range dependencies, as well as pairwise relations among human body parts and joints in the
spatial and temporal domains. The cross-attention module learns view-invariant attention maps and generates discriminative
features for semantic representations of actions in different views.
